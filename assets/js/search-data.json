{
  
    
        "post0": {
            "title": "In-context learning thread",
            "content": "Acknowledgements This work is part of the result of my CHERI Summer Research Project. I am grateful to the CHERI for giving me this opportunity to do my first AI Safety research in such a nice environment! . . Definitions of in-context learning . Why does in-context learning matters? . In-context learning and generalization . Phase transition and in-context learning . Concurrent theories . Induction heads . Parameters norm growth and saturation . Experiments and results .",
            "url": "https://ppeigne.github.io/algorithmic_meditations/markdown/in-context-learning/2022/09/01/in-context-learning-thread.html",
            "relUrl": "/markdown/in-context-learning/2022/09/01/in-context-learning-thread.html",
            "date": " • Sep 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "What is 'in-context learning'?",
            "content": "Acknowledgements:This work is part of the result of my CHERI Summer Research Project.Many thanks to my mentor Asa Cooper Stickland for his advices on this project,and to the CHERI for giving me this opportunity to do my first AI Safety research in such a nice environment! . Key takeaways . In-context learning can be defined as the property that, given a prompt of text, later tokens are easier to predict than earlier ones. | This property requieres the presence of novelty in the run-time environment to be observed. | The novelty can be introduced in the run-time environment using orthogonal strategies: from the introduction of new entities. | from the introduction of a new distribution of the known entities. | . | . . Definition . Terminology alert! . The building blocks of natural language processing are not words, but tokens. A token is a sequence of characters from which words are composed. The set of all tokens is the vocabulary of the model. The training of the model will produce a specific vocabulary based on the type of text it is trained with. For example, the word “emergence” could be divided into [em][er][ge][nce] (or any other combination of characters that is most useful to the model). . Broad definition . Discovered with GPT2, in-context learning is a key emergent property of Large Language Models. In-context learning can be defined as the property that, given a prompt of text, later tokens are easier to predict than earlier ones. . For instance, the model’s prediction of what follows [hog] will dramatically increase toward [wart] if it is prompted with an extract of the Harry Potter novel (assuming the prompted text contains a mention of the Hogwart school of wizardry), even though the model was trained on a textual corpus that never mentioned the Harry Potter universe (e.g. Shakespeare poems). . . In other words, our model will learn to write in the style of J.K. Rowling from its prompt while having been only trained to imitate Shakespeare. . This phenomenon is therefore called “in-context learning” because the information from context (i.e. the previous part of the prompt) can be used by the model to improve its prediction. The longer the context the better the prediction. . Novelty . On its most basic level, in-context learning is defined (for a model being given a text prompt as input) as the property of being able to produce a better prediction for the later tokens of the prompt than for the earlier ones. This improvement is a result of the model being able to use the information present in the prompt (also called the context), i.e. “to learn from context”. . This ability implies that the task from the context cannot be performed well only relying on the model’s previous learning. . However, starting from this definition it is possible to produce different interpretations of what “learning from context” means. To make sure the model does not rely on learning from its training to perform the task, novelty must be injected into the prompted task. . Orthogonal interpretations rely on a different approach to how to bring novelty into the context: . Novelty of the entities: the context’s entities are unknown to the model (i.e. never been encountered in the pre-training phase). | Novelty of the tokens distribution: the model is given an unusual task based on an unusual distribution of tokens. | . ‘New entities based’ in-context learning (Chan et al. 2022) . The gist of this approach is that asking the model to adapt to completely new entities (i.e. new tokens) implies that it cannot rely on what it has learned before to perform the task. This task can then be considered “new” for the model. . Example: the incomplete (but consistent) English-Korean mapping . Imagine an experiment where some english characters are systematically replaced by the closest sounding Korean characters: [n] -&gt; [ㄴ], [k] -&gt; [ㅋ], [o] -&gt; [ㅗ], etc. . The replacement is consistent: each english character is always replaced by the same korean character. | The replacement is incomplete: not every english character are replaced. | . After the replacement, the modified text is used as a prompt for the model trained on the Shakespear corpus. The model is expected to output text in the style of Shakespeare but with the same English-Korean mapping it got as an input. . Training data Context data . “To be or not to be…” | 1“To be ㅗr ㄴㅗt to be…“New entities | . ‘New Distribution-based’ in-context learning (Xie et al. 2022) . The gist of this approach is that asking the model to adapt to a specific distribution of tokens different enough from its training data distribution that it cannot rely on what it has learned before to perform the task. This task can then be considered as “new” for the model. . The example of Harry Potter is an example of a new distribution of tokens. The entities are the same, but their distribution is different. For instance, the token [wart] already exists in the Shakespeare vocabulary but is not used after [hog]. Based on the training data distribution, the probability P([wart] | …[hog]) is close to zero. . Training data Context data . “To be or not to be…” | “Don’t let the muggles get you down.”New distribution | . Mapping novelty . We can connect those orthogonal dimensions to evaluate claims about in-context learning. . This bi-dimensional classification also highlights possible experimental setups combining novelty of the distribution (task) and of the entities, which have not been investigated yet. .   New entities Same entities . New distribution | “잠의 들판으로”2 | “Don’t let the muggles get you down.” | . Same distribution | “To be ㅗr ㄴㅗt to be…” | “To be or not to be…” | . References . Footnotes . This is Shakespeare with a korean hat, not Guy Fawkes. &#8617; . | Korean poem, “Toward the Field of the Sleep” &#8617; . |",
            "url": "https://ppeigne.github.io/algorithmic_meditations/in-context-learning/cheri/2022/08/31/in-context-learning.html",
            "relUrl": "/in-context-learning/cheri/2022/08/31/in-context-learning.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ppeigne.github.io/algorithmic_meditations/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ppeigne.github.io/algorithmic_meditations/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ppeigne.github.io/algorithmic_meditations/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}