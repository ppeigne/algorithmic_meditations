<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>What is ‘in-context learning’? | Algorithmic Meditations</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="What is ‘in-context learning’?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An exploration of the concept of in-context learning" />
<meta property="og:description" content="An exploration of the concept of in-context learning" />
<link rel="canonical" href="https://ppeigne.github.io/algorithmic_meditations/in-context-learning/cheri/2022/08/31/in-context-learning.md.copy" />
<meta property="og:url" content="https://ppeigne.github.io/algorithmic_meditations/in-context-learning/cheri/2022/08/31/in-context-learning.md.copy" />
<meta property="og:site_name" content="Algorithmic Meditations" />
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/c/cd/Giulio_Rosati_10.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-31T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://upload.wikimedia.org/wikipedia/commons/c/cd/Giulio_Rosati_10.jpg" />
<meta property="twitter:title" content="What is ‘in-context learning’?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-31T00:00:00-05:00","datePublished":"2022-08-31T00:00:00-05:00","description":"An exploration of the concept of in-context learning","headline":"What is ‘in-context learning’?","image":"https://upload.wikimedia.org/wikipedia/commons/c/cd/Giulio_Rosati_10.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://ppeigne.github.io/algorithmic_meditations/in-context-learning/cheri/2022/08/31/in-context-learning.md.copy"},"url":"https://ppeigne.github.io/algorithmic_meditations/in-context-learning/cheri/2022/08/31/in-context-learning.md.copy"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/algorithmic_meditations/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ppeigne.github.io/algorithmic_meditations/feed.xml" title="Algorithmic Meditations" /><link rel="shortcut icon" type="image/x-icon" href="/algorithmic_meditations/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/algorithmic_meditations/">Algorithmic Meditations</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/algorithmic_meditations/about/">About Me</a><a class="page-link" href="/algorithmic_meditations/search/">Search</a><a class="page-link" href="/algorithmic_meditations/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">What is &#39;in-context learning&#39;?</h1><p class="page-description">An exploration of the concept of in-context learning</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-08-31T00:00:00-05:00" itemprop="datePublished">
        Aug 31, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/algorithmic_meditations/categories/#in-context-learning">in-context-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/algorithmic_meditations/categories/#CHERI">CHERI</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    ![](/algorithmic_meditations/images/Copy%20of%20CHERI_LogoV1.png){: style="float: left"} ***Acknowledgements:***<br>*This work is part of the result of my [CHERI Summer Research Project](https://effectivealtruism.ch/2022-summer-research-program).<br>Many thanks to my mentor [Asa Cooper Stickland](https://homepages.inf.ed.ac.uk/s1302760/) for his advices on this project,<br>and to the CHERI for giving me this opportunity to do my first AI Safety research in such a nice environment!*

# Key takeaways
- In-context learning can be defined as the property that, **given a prompt of text**, **later tokens are easier to predict than earlier ones**.
- This property requieres the **presence of novelty in the run-time environment** to be observed.
- The novelty can be introduced in the run-time environment using orthogonal strategies:
  - from the introduction of **new entities**.
  - from the introduction of a **new distribution of the known entities**.

![The discussion, Guilio Rosati](/algorithmic_meditations/images/conversation.png)

# Definition

### Tokens
### Context

## Broad definition
Discovered with GPT2, in-context learning is a key emergent property of Large Language Models. In-context learning can be defined as the property that, **given a prompt of text**, **later tokens are easier to predict than earlier ones**. 

For instance, **the model’s prediction of what follows [hog] will dramatically increase toward [wart] if it is prompted with an extract of the Harry Potter novel** (assuming the prompted text contains a mention of the Hogwart school of wizardry), **even though the model was trained on a textual corpus that never mentioned the Harry Potter universe** (e.g. Shakespeare poems). 

![](/algorithmic_meditations/images/Screenshot from 2022-08-31 14-34-51.png)

In other words, our model will learn to write in the style of J.K. Rowling from its prompt while having been only trained to imitate Shakespeare. 

This phenomenon is therefore called “in-context learning” because the information from context (i.e. the previous part of the prompt) can be used by the model to improve its prediction. **The longer the context the better the prediction.**

## Novelty
On its most basic level, in-context learning is defined (for a model being given a text prompt as input) as the property of being able to produce a better prediction for the later tokens of the prompt than for the earlier ones. This improvement is a result of the model being able to use the information present in the prompt (also called the context), i.e. "to learn from context".

This ability implies that **the task from the context cannot be performed well only relying on the model’s previous learning**. 

However, starting from this definition it is possible to produce different interpretations of what "learning from context" means. To make sure the model does not rely on learning from its training to perform the task, **novelty must be injected into the prompted task**. 

Orthogonal interpretations rely on a different approach to how to bring novelty into the context:
- **Novelty of the entities**: the context's entities are unknown to the model (i.e. never been encountered in the pre-training phase).
- **Novelty of the tokens distribution**: the model is given an unusual task based on an unusual distribution of tokens. 


## Narrow definitions

### New entities / ‘Knowledge-based' in-context learning (Chan et al. 2022)
**Is in-context learning based on the same entities as the training phase?**

- Known: Yes. The distribution of entities among the data can change but none of them are completely new to the model. 
For instance, the token [wart] already exists in the Shakespeare vocabulary but is not used after [hog]: based on the training data distribution, the probability P([wart] | …[hog]) is close to zero. 

- New:  No. The entities are completely new to the model.
For instance, the model trained on Shakespeare’s poems gets fed in Korean characters sequences to complete.

The gist of this approach is that asking the model to adapt to completely new tokens (i.e. new entities) from its training data distribution implies that it cannot rely on what it has learned before to perform the task. This task can then be considered “new” for the model. 

Example from Chan et al. (2022):
The model is trained to map images to labels (hand-written letters with a corresponding numerical label). Then the model is asked to predict the label of images unseen during the training (but given in the context). 

||Training|In-prompt|
|-|-|-|
|**Known data: Xie et al. (2022)**|The model is trained to predict the next token of a document, based on **a fixed vocabulary**. <br><br> *“Marie Salomea Skłodowska–Curie; born Maria Salomea Skłodowska, 7 November 1867 – 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize…”*|The model is asked to complete (i.e. predict the next token) a prompt having a very unusual distribution of entities (i.e. a task different from the original training task) from **the same known vocabulary.** <br><br> *“Albert Einstein was German<br> Mahatma Gandhi was Indian <br> Marie Curie was”*|
|**New data: Chan et al. (2022)**|The model is trained on image-labels pairs to predict the label of the last image. <br> ![](/algorithmic_meditations/images//Screenshot%20from%202022-08-31%2014-21-15.png)|The model is prompted with a sequence of **totally new image-label pairs** to predict the label of the last image. <br> ![](/algorithmic_meditations/images//Screenshot%20from%202022-08-31%2014-29-49.png)|


### New distribution / 'Distribution-based' in-context learning (Xie et al. 2022)
**Is the in-context learning based on the same data distribution (task) as the pre-training phase?**

- Same: Yes. The distribution of data is the same, hence the task is the same.
For instance, the model gets fed in with the beginning of an unknown (i.e. not present in the training data) Shakespeare’s poem to complete. The distribution of tokens is supposed to be very similar to what the model was trained with. 

- Unusual: No. The task is somewhat different from the pre-training task because the distribution of tokens is different.
For example, the model trained on Shakespeare gets fed in with an extract of a Harry Potter novel to complete. If the distribution of tokens is different from the training data, then the task has changed. 

The gist of this approach is that asking the model to adapt to a specific distribution of tokens different enough from its training data distribution implies that it cannot rely on what it has learned before to perform the task. This task can then be considered as “new” for the model. 

Example from Xie et al. (2022): 
Training a model on data from Wikipedia will lead the model to learn a distribution of tokens that are linked together following specific statistics. A Wikipedia article is usually focused mainly on one subject, says Marie Curie, over an entire document composed of dozens, or even hundreds of sentences.  

On the other hand, the in-prompt task will produce a completely different distribution to follow.  
*“Albert Einstein was German  
Mahatma Gandhi was Indian  
Marie Curie was”*  
-> *“Polish”.*

In such a setting, the distribution shifts very quickly from one subject (Albert Einstein, Mahatma Gandhi, Marie Curie) to another in a way that is very unlikely (but not impossible) to appear in the training data.

More formally, we could say that P(“Mahatma” \| “Albert Einstein was German”) is very low given the model’s training distribution.


||Training|In-prompt|
|-|-|-|
|**Unusual distribution: Xie et al. (2022)**|The model is trained to predict the next token of a document, based on **a fixed vocabulary**. <br><br> *“Marie Salomea Skłodowska–Curie; born Maria Salomea Skłodowska, 7 November 1867 – 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize…”*|The model is asked to complete (i.e. predict the next token) a prompt having a **very unusual distribution of entities** (i.e. a task different from the original training task) from the same known vocabulary. <br><br> *“Albert Einstein was German<br> Mahatma Gandhi was Indian <br> Marie Curie was”*|
|**Same distribution: Chan et al. (2022)**|The model is trained on image-labels pairs to predict the label of the last image. <br> ![](/algorithmic_meditations/images/Screenshot%20from%202022-08-31%2014-21-15.png)|The model is prompted with a sequence of totally new image-label pairs to predict the label of the last image. <br> ![](/algorithmic_meditations/images/Screenshot%20from%202022-08-31%2014-29-49.png)|

### Mapping novelty
We can connect those orthogonal dimensions to evaluate claims about in-context learning. 
This bi-dimensional classification also highlights possible experimental setups combining novelty of the distribution (task) and of the entities, which have not been investigated yet.

||New Entities|Known Entities|
|-|-|-|
|**Unusual distribution**|**Not explored yet**<br>Training:<br> *“Marie Salomea Skłodowska–Curie; born Maria Salomea Skłodowska, 7 November 1867 – 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize…”* <br>Prompt:<br> *“Albert Einstein was German<br> [new_token1] [new_token2] was [new_token3] <br> Marie Curie was”*|Xie et al. (2022) <br>**Classic in-prompt instructions**<br>Training:<br> *“Marie Salomea Skłodowska–Curie; born Maria Salomea Skłodowska, 7 November 1867 – 4 July 1934) was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity. She was the first woman to win a Nobel Prize…”* <br>Prompt:<br> *“Albert Einstein was German<br> Mahatma Gandhi was Indian <br> Marie Curie was”*|
|**Same distribution**|Chan et al. (2022)<br> Training:<br> ![](/algorithmic_meditations/images/Screenshot%20from%202022-08-31%2014-21-15.png)<br> Prompt:<br> ![](/algorithmic_meditations/images/Screenshot%20from%202022-08-31%2014-29-49.png)|No novelty|


## References



# Definition
### Terminology alert!
The building blocks of natural language processing are not words, but **tokens**. A token is a **sequence of characters from which words are composed**. **The set of all tokens is the vocabulary** of the model. The training of the model will produce a specific vocabulary based on the type of text it is trained with.  
*For example, the word “emergence” could be divided into [em][er][ge][nce] (or any other combination of characters that is most useful to the model).*




## A simple definition





## Concurent explanations
Until being properly explained, emergent behaviors point by definition to weaknesses in the theoretical framework. Surprising events are surprising only to the extent that they are not clearly understood.

The emergence of in-prompt learning is still not well understood and subject to concurrent hypotheses. 

For instance (not being comprehensive): 
- Xie et al. (2022) claim that Transformers are the only architecture able to display in-context learning, while Chan et al. (2022) experiment on a synthetic dataset pretends to make in-prompt learning feasible for both Transformers and LSTMs. Both papers also mentioned datasets' specific properties to make in-prompt learning happen.  

- Anthropic’s team in its Transformer Circuit thread proposed an approach based on a mechanistic interpretation of transformers models which led to the discovery of ‘induction heads’. According to the theoretical framework, they are developing, these elements are supposed to be core components leading to in-context learning abilities.

- Another approach suggests that the transformers' impressive abilities (including in-context learning) could come from saturation of their parameters during their training. 

## Narrower definitions of in-context learning




  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ppeigne/algorithmic_meditations"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/algorithmic_meditations/in-context-learning/cheri/2022/08/31/in-context-learning.md.copy" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/algorithmic_meditations/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/algorithmic_meditations/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/algorithmic_meditations/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An AI Safety research blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
